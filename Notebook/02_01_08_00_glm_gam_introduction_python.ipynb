{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/01_Generalized_Linear_Models_Python/blob/main/Notebook/02_01_08_00_glm_gam_introduction_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu_crfiZLczD"
      },
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1IFEWet-Aw4DhkkVe1xv_2YYqlvRe9m5_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Generalized Additive Model (GAM)\n",
        "\n",
        "Generalized Additive Models (GAMs) are powerful tools for modeling complex, nonlinear relationships in data. They combine the flexibility of nonparametric models with the interpretability of linear models. This tutorial will guide you through the fundamentals of GAMs in R. To enhance your understanding, we will build a GAM model from scratch without using any external packages. This approach will illustrate the core principles behind GAM modeling, including smoothing, combining predictor effects, and estimating model parameters. We will also explore various packages for fitting, analyzing, and visualizing GAMs, including popular libraries such as {mgcv} and {gam}. These packages provide robust functions to fit GAMs, offering flexibility with multiple types of smoothers, diagnostics, and model selection criteria. Additionally, we will delve into specific packages designed for GAM visualization and model assessment, equipping you with tools to evaluate and interpret complex relationships in your data.\n",
        "\n",
        "By the end of this tutorial, you will have a comprehensive understanding of how to use GAMs in R to uncover intricate relationships in your data. You will gain practical skills in fitting, interpreting, and diagnosing GAM models, along with insights into the mathematical principles behind them."
      ],
      "metadata": {
        "id": "J42bCd1ZNH-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "A Generalized Additive Model (GAM) is an extension of traditional linear regression models that allows for more flexibility by modeling the relationship between the response variable and each predictor variable as a smooth, non-linear function. This flexibility makes GAMs especially useful when relationships between predictors and the outcome are complex and cannot be adequately captured by a linear model.\n",
        "\n"
      ],
      "metadata": {
        "id": "o55QjJtZNyeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Structure of a GAM\n",
        "\n",
        "A Generalized Additive Model (GAM) is defined as:\n",
        "\n",
        "$$ g(E(y)) = \\beta_0 + f_1(x_1) + f_2(x_2) + \\dots + f_n(x_n) $$\n",
        "\n",
        "where:\n",
        "\n",
        "-   $y$ is the dependent variable, or the outcome we’re predicting.\n",
        "-   $E(y)$ represents the expected value of $y$.\n",
        "-   $E(y)$ represents the expected value of $y$.\n",
        "-   $g$ is a link function that links the predictors to the expected value of $y$.\n",
        "-   $\\beta_0$ is the intercept, representing the baseline value of $y$.\n",
        "-   $f_1(x_1), f_2(x_2), \\dots, f_n(x_n)$ are smooth, flexible functions for each predictor variable $x_1, x_2, \\dots, x_n$.\n",
        "\n",
        "In GAMs, instead of assuming each predictor affects the outcome linearly, each predictor has its own flexible function, allowing it to influence the outcome in potentially complex, nonlinear ways."
      ],
      "metadata": {
        "id": "J3f9UTBiN4jW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Components of a GAM\n",
        "\n",
        "To better understand GAMs, let's look at the main components:\n",
        "\n",
        "a.  **Non-Parametric  Smooth Functions** $f_i(x_i)$\n",
        "\n",
        "Each predictor $x_i$ has its own smooth function, $f_i(x_i)$, which is designed to capture the potentially nonlinear relationship between $x_i$ and $y$). These functions are often estimated using methods like:\n",
        "\n",
        "-   **Splines**: Splines (e.g., cubic splines) are piecewise polynomials joined smoothly at certain points (called knots). They allow for a smooth curve without specifying an exact form for the relationship.\n",
        "-   **Local Regression (LOESS/LOWESS)**: A non-parametric regression technique that fits simple models to localized subsets of data, offering a smooth curve without needing a specific functional form.\n",
        "-   **Kernel Smoothing**: Kernel-based methods allow estimating smooth functions by averaging nearby points, with the weights determined by a kernel function.\n",
        "\n",
        "These smoothing methods help model the relationship between each predictor and the outcome in a flexible, data-driven way.\n",
        "\n",
        "b.  **Generalized Framework (Link Function)** $g$\n",
        "\n",
        "The link function $g$ relates the predictors to the expected value of $y$. Some commonly used link functions are:\n",
        "\n",
        "-   **Identity Link**: $g(y) = y$, used for continuous outcomes (e.g., linear regression).\n",
        "\n",
        "-   **Log Link**: $g(y) = \\ln(y)$, used for modeling positive, skewed outcomes like counts.\n",
        "\n",
        "-   **Logit Link**: $g(y) = \\ln\\left(\\frac{y}{1 - y}\\right)$, used for binary or proportion data.\n",
        "\n",
        "By choosing an appropriate link function, GAMs can model different types of outcome distributions (continuous, binary, count data, etc.).\n",
        "\n",
        "c.  **Additivity Assumption**\n",
        "\n",
        "GAMs assume that each predictor contributes independently to the outcome, meaning there are no interactions between predictors (although it’s possible to add interaction terms). This additivity makes GAMs interpretable because we can examine the effect of each predictor individually."
      ],
      "metadata": {
        "id": "7E3xFIsnN_gG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimation and Fitting GAMs\n",
        "\n",
        "To estimate a GAM, the following steps are typically involved:\n",
        "\n",
        "a.  **Choosing the Smoothness of** $f_i(x_i)$\n",
        "\n",
        "Each smooth function $f_i(x_i)$ needs to be tuned for “smoothness.” If $f_i(x_i)$ is too flexible, the model might overfit the data, capturing noise rather than true relationships. Conversely, if $f_i(x_i)$ is too rigid, it may miss important trends. Regularization methods, such as penalizing the complexity of $f_i(x_i)$, help control this balance. The degree of smoothness is often chosen by minimizing a model selection criterion like **Generalized Cross-Validation (GCV)** or **Akaike Information Criterion (AIC)**.\n",
        "\n",
        "b.  **Estimating Coefficients**\n",
        "\n",
        "The coefficients $\\beta_0$ and the functions $f_i(x_i)$ are estimated by maximizing the likelihood of the model (or minimizing a loss function). This is often done using iterative algorithms, like backfitting, that alternate between fitting each function while keeping the others fixed until convergence.\n",
        "\n",
        "c.  **Diagnostics and Model Evaluation**\n",
        "\n",
        "Once fitted, a GAM can be evaluated using: - **Residual analysis**: Plotting residuals to check for patterns, which can indicate model misfit. - **Cross-validation**: Splitting data into training and test sets to check predictive performance. - **Model selection criteria**: Metrics like AIC or GCV to compare different model specifications."
      ],
      "metadata": {
        "id": "B4b71B3eODUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applications of GAMs\n",
        "\n",
        "a.  **Ecology and Environmental Science**\n",
        "\n",
        "In ecology, GAMs are often used to study the relationship between species abundance and environmental factors (e.g., temperature, rainfall). For example, one might model how fish population changes with water temperature, salinity, and nutrient levels in a non-linear manner.\n",
        "\n",
        "b.  **Economics**\n",
        "\n",
        "In economics, GAMs can model relationships that are not strictly linear, like how consumer spending varies with income level and Income. GAMs allow each of these factors to influence spending in complex, nonlinear ways.\n",
        "\n",
        "c.  **Medicine and Public Health**\n",
        "\n",
        "In medical research, GAMs are used to model the effects of Income, dosage levels, or other health metrics on patient outcomes, where the relationship might be nonlinear (e.g., the effect of dosage on blood pressure might increase up to a point and then level off).\n",
        "\n",
        "d.  **Marketing and Social Science**\n",
        "\n",
        "In marketing, GAMs are useful to understand how advertising spend, customer demographics, and other factors impact customer engagement or sales, which often have nonlinear effects.\n"
      ],
      "metadata": {
        "id": "Weel96cqOMt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalized Additive Models (GAMs) in Python\n",
        "\n",
        "While Python doesn’t have as many native GAM libraries as R, several powerful packages enable fitting Generalized Additive Models with flexibility and performance. Below is a guide to the most popular Python libraries for GAMs:\n",
        "\n",
        "### 1. **`pyGAM`**\n",
        "\n",
        "- **`pyGAM`** is the most comprehensive and widely used Python library for fitting GAMs. It is inspired by R’s `mgcv` and supports penalized B-splines, P-splines, and other smooth terms.\n",
        "\n",
        "- **Key Features:**\n",
        "  - Offers a variety of spline terms: `s()` for univariate splines, `f()` for factors, `l()` for linear terms, and `te()` for tensor products (multivariate smooths).\n",
        "  - Supports multiple distributions and link functions via the `distribution=` and `link=` parameters.\n",
        "  - Built-in cross-validation for smoothing parameter selection.\n",
        "  - Uses `scipy` and `numpy` under the hood; integrates well with the scientific Python stack.\n",
        "\n",
        "- **Example:**\n",
        "  ```python\n",
        "  from pygam import LinearGAM, s, f\n",
        "  from pygam.datasets import wage\n",
        "\n",
        "  X, y = wage()\n",
        "  gam = LinearGAM(s(0) + s(1) + f(2)).fit(X, y)\n",
        "  gam.summary()\n",
        "  ```\n",
        "\n",
        "\n",
        "### 2. **`statsmodels` (Limited GAM Support)**\n",
        "\n",
        "- While `statsmodels` doesn’t have full native GAM functionality, it supports **Generalized Linear Models (GLMs)** and **splines via basis functions** (e.g., using `bs()` or `cr()` from `patsy`).\n",
        "\n",
        "- **Key Features:**\n",
        "  - You can manually construct spline basis expansions and fit them in a GLM framework.\n",
        "  - Good for simple additive models with pre-specified knots or degrees of freedom.\n",
        "  - Lacks automatic smoothing parameter selection and penalization found in `pyGAM` or R’s `mgcv`.\n",
        "\n",
        "- **Example:**\n",
        "  ```python\n",
        "  import statsmodels.api as sm\n",
        "  import patsy\n",
        "\n",
        "  # Create spline basis using patsy\n",
        "  y, X = patsy.dmatrices(\"wage ~ bs(age, df=5) + education\", data=df, return_type='dataframe')\n",
        "  model = sm.GLM(y, X, family=sm.families.Gaussian()).fit()\n",
        "  print(model.summary())\n",
        "  ```\n",
        "\n",
        "\n",
        "### 3. **`scikit-learn` + Custom Splines (Manual Approach)**\n",
        "\n",
        "- `scikit-learn` does not directly support GAMs, but you can approximate them by:\n",
        "  - Using `SplineTransformer` (available in scikit-learn ≥ 1.0) to generate spline basis features.\n",
        "  - Fitting a penalized linear model (e.g., `Ridge`, `Lasso`) or GLM via `sklearn.linear_model`.\n",
        "\n",
        "- **Key Features:**\n",
        "  - Good for educational or prototyping purposes.\n",
        "  - Requires manual tuning of knots and penalties.\n",
        "  - No built-in distribution families (beyond Gaussian with linear models).\n",
        "\n",
        "- **Example:**\n",
        "  ```python\n",
        "  from sklearn.preprocessing import SplineTransformer\n",
        "  from sklearn.linear_model import Ridge\n",
        "  from sklearn.pipeline import Pipeline\n",
        "\n",
        "  spline = SplineTransformer(n_knots=10, degree=3)\n",
        "  model = Pipeline([\n",
        "      ('spline', spline),\n",
        "      ('ridge', Ridge(alpha=1.0))\n",
        "  ])\n",
        "  model.fit(X.reshape(-1, 1), y)\n",
        "  ```\n",
        "\n",
        "\n",
        "### 4. **`GAMboost` / `pyGAM` Alternatives (Less Common)**\n",
        "\n",
        "- Libraries like `GAMboost` (via `rpy2` wrapper) or `interpret.glassbox` (from Microsoft’s InterpretML) offer GAM-like models, especially for interpretability.\n",
        "\n",
        "- **InterpretML’s ExplainableBoostingMachine (EBM):**\n",
        "  - Not strictly a statistical GAM, but fits additive models using boosting.\n",
        "  - Excellent for high-dimensional, interpretable models.\n",
        "  - Automatically handles interactions and provides visual explanations.\n",
        "\n",
        "  ```python\n",
        "  from interpret.glassbox import ExplainableBoostingRegressor\n",
        "  from interpret import show\n",
        "\n",
        "  ebm = ExplainableBoostingRegressor()\n",
        "  ebm.fit(X, y)\n",
        "  show(ebm.explain_global())\n",
        "  ```\n",
        "\n",
        "."
      ],
      "metadata": {
        "id": "qe55GnI7Ob5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conclusions\n",
        "\n",
        "A Generalized Additive Model (GAM) is a powerful extension of GLMs that uses smooth functions to model non-linear relationships in a flexible yet interpretable way. It strikes a balance between parametric models and fully non-parametric or black-box models (like neural networks), making it popular in ecology, medicine, finance, and social sciences."
      ],
      "metadata": {
        "id": "DCI0dXetOhA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resources\n",
        "\n",
        "1. pyGAM — Best for stats-style GAMs (like R’s mgcv)  \n",
        "  Docs: https://pygam.readthedocs.io  \n",
        "\n",
        "2. InterpretML (EBM) — Best for interpretable ML & auto-GAMs**  \n",
        "  Site: https://interpret.ml  \n",
        "\n",
        "3. ISLR Python Examples — Learn theory + code   \n",
        "→ GitHub: https://github.com/JWarmenhoven/ISLR-python  \n",
        "\n",
        "4. StatQuest Video — GAMs in <15 mins\n",
        " Link: https://youtu.be/8BoWqSOvMnY  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7xBtQkhvOlwc"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "4.2.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}